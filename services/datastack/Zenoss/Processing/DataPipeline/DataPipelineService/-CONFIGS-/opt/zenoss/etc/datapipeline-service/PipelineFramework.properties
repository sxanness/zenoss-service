# Use to override datapipeline-framework defaults as necessary.
# refresh interval in seconds
application.streaming.interval=20

zookeeper.ensemble=zk1:2181
bootstrap.servers=kfk0:9092
schema.registry.url=http://dsr0:8081
spark.master=spark://spm0:7077

#spark properties
spark.home=/opt/spark
spark.executor.memory=512m
spark.driver.memory=512m
spark.eventLog.enabled=true
# The following must be DFS, preferably HDFS
spark.eventlog.dir=/var/zenoss/datapipeline/run/spark-events

#Hook up Hadoop classpath here
#spark.driver.extraClassPath=
#spark.max.cores=6
#spark.streaming.backpressure.enabled=true
#spark.streaming.kafka.maxRatePerPartition=

#Global values (i.e. defaults for all pipelines) for Kafka
auto.offset.reset=largest

#Stage specific properties that override global defaults. These should typically be provided in the pipeline configs
#pipeline.source.text.input.dir=
#pipeline.source.databus.topics=
#pipeline.source.databus.bootstrap.servers=
# smallest: start from beginning, largest: start from end
#pipeline.source.databus.startoffset=largest


# Framework config
framework.dir.log=/opt/zenoss/log/datapipeline
# The scratch dir must be a local FS for fast writes of temporary Spark data
framework.dir.scratch=/tmp
# The checkpoint dir should be a DFS location
framework.dir.checkpoint=/var/zenoss/datapipeline/run/checkpoint

pipeline.dir.static=/opt/zenoss/etc/datapipeline/pipelines
pipeline.dir.dynamic=/var/zenoss/datapipeline/etc/pipelines