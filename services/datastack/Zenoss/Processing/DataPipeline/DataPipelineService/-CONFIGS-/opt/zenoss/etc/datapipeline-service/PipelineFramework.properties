# Use to override datapipeline-framework defaults as necessary.
# refresh interval in seconds
application.streaming.interval=20

zookeeper.ensemble=172.31.16.14:5181,172.31.17.9:5181,172.31.27.159:5181
bootstrap.servers=172.31.43.108:9092,172.31.43.216:9092,172.31.45.107:9092
schema.registry.url=http://172.31.43.108:8081,http://172.31.43.216:8081,http://172.31.45.107:8081
spark.master=spark://172.31.16.14:7077,172.31.17.9:7077,172.31.27.159:7077

#spark properties
spark.home=/opt/spark
spark.executor.memory=512m
spark.driver.memory=512m
spark.eventLog.enabled=true
# The following must be DFS, preferably HDFS
spark.eventlog.dir=/var/zenoss/datapipeline/run/spark-events

#Hook up Hadoop classpath here
#spark.driver.extraClassPath=
#spark.max.cores=6
#spark.streaming.backpressure.enabled=true
#spark.streaming.kafka.maxRatePerPartition=

#Global values (i.e. defaults for all pipelines) for Kafka
auto.offset.reset=largest

#Stage specific properties that override global defaults. These should typically be provided in the pipeline configs
#pipeline.source.text.input.dir=
#pipeline.source.databus.topics=
#pipeline.source.databus.bootstrap.servers=
# smallest: start from beginning, largest: start from end
#pipeline.source.databus.startoffset=largest


# Framework config
framework.dir.log=/opt/zenoss/log/datapipeline
# The scratch dir must be a local FS for fast writes of temporary Spark data
framework.dir.scratch=/tmp
# The checkpoint dir should be a DFS location
framework.dir.checkpoint=/var/zenoss/datapipeline/run/checkpoint

pipeline.dir.static=/opt/zenoss/etc/datapipeline/pipelines
pipeline.dir.dynamic=/var/zenoss/datapipeline/etc/pipelines